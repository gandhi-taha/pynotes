{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we will work with NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is easy to compute the lentgth of a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary scope is obtained by removing the duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19317"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question that almost suggests itself is how often each of these tokens occurs in the text. Such a\n",
    "list (sorted in ascending order) of tokens and their frequencies is also called a frequency list.\n",
    "\n",
    "NLTK provides a function for generating frequency lists (also called frequency distributions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 18713, 'the': 13721, '.': 6862, 'of': 6536, 'and': 6024, 'a': 4569, 'to': 4542, ';': 4072, 'in': 3916, 'that': 2982, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1 = FreqDist(text1)\n",
    "\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The n most frequent tokens of a text are obtained with the help of the function most common():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982),\n",
       " (\"'\", 2684),\n",
       " ('-', 2552),\n",
       " ('his', 2459),\n",
       " ('it', 2209),\n",
       " ('I', 2124),\n",
       " ('s', 1739),\n",
       " ('is', 1695),\n",
       " ('he', 1661),\n",
       " ('with', 1659),\n",
       " ('was', 1632),\n",
       " ('as', 1620),\n",
       " ('\"', 1478),\n",
       " ('all', 1462),\n",
       " ('for', 1414),\n",
       " ('this', 1280),\n",
       " ('!', 1269),\n",
       " ('at', 1231),\n",
       " ('by', 1137),\n",
       " ('but', 1113),\n",
       " ('not', 1103),\n",
       " ('--', 1070),\n",
       " ('him', 1058),\n",
       " ('from', 1052),\n",
       " ('be', 1030),\n",
       " ('on', 1005),\n",
       " ('so', 918),\n",
       " ('whale', 906),\n",
       " ('one', 889),\n",
       " ('you', 841),\n",
       " ('had', 767),\n",
       " ('have', 760),\n",
       " ('there', 715),\n",
       " ('But', 705),\n",
       " ('or', 697),\n",
       " ('were', 680),\n",
       " ('now', 646),\n",
       " ('which', 640),\n",
       " ('?', 637),\n",
       " ('me', 627),\n",
       " ('like', 624)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how often the word whale occurs in Moby Dick, we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " fdist1[\"whale\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the text also contains title and author information and there is neither a normalisation of the spelling (upper/lower case) nor a filtering of non-lexical tokens. Again, the result\n",
    "changes if we convert all tokens of the text to lower case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 18713, 'the': 14431, '.': 6862, 'of': 6609, 'and': 6430, 'a': 4736, 'to': 4625, 'in': 4172, ';': 4072, 'that': 3085, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2 = FreqDist([d.lower() for d in text1])\n",
    "fdist2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hapaxes()\n",
    "\n",
    "hapaxes() would return a list of words that occur only once in that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['herman',\n",
       " 'melville',\n",
       " ']',\n",
       " 'etymology',\n",
       " 'consumptive',\n",
       " 'threadbare',\n",
       " 'lexicons',\n",
       " 'mockingly',\n",
       " 'flags',\n",
       " 'mortality',\n",
       " 'signification',\n",
       " 'sw',\n",
       " 'hval',\n",
       " 'roundness',\n",
       " 'dut',\n",
       " 'ger',\n",
       " 'wallen',\n",
       " 'walw',\n",
       " 'ian',\n",
       " 'richardson',\n",
       " 'ketos',\n",
       " 'whoel',\n",
       " 'anglo',\n",
       " 'wal',\n",
       " 'hwal',\n",
       " 'swedish',\n",
       " 'baleine',\n",
       " 'ballena',\n",
       " 'fegee',\n",
       " 'erromangoan',\n",
       " 'librarian',\n",
       " 'painstaking',\n",
       " 'burrower',\n",
       " 'vaticans',\n",
       " 'stalls',\n",
       " 'higgledy',\n",
       " 'piggledy',\n",
       " 'promiscuously',\n",
       " 'commentator',\n",
       " 'belongest',\n",
       " 'sallow',\n",
       " 'sherry',\n",
       " 'loves',\n",
       " 'bluntly',\n",
       " 'subs',\n",
       " 'thankless',\n",
       " 'hampton',\n",
       " 'hie',\n",
       " 'refugees',\n",
       " 'pampered',\n",
       " 'michael',\n",
       " 'raphael',\n",
       " 'unsplinterable',\n",
       " 'punish',\n",
       " 'soever',\n",
       " 'cometh',\n",
       " 'incontinently',\n",
       " 'perisheth',\n",
       " 'plutarch',\n",
       " 'morals',\n",
       " 'breedeth',\n",
       " 'whirlpooles',\n",
       " 'balaene',\n",
       " 'arpens',\n",
       " 'tooke',\n",
       " 'lucian',\n",
       " 'catched',\n",
       " 'octher',\n",
       " '890',\n",
       " 'gudgeon',\n",
       " 'retires',\n",
       " 'montaigne',\n",
       " 'apology',\n",
       " 'raimond',\n",
       " 'sebond',\n",
       " 'cartloads',\n",
       " 'stowe',\n",
       " 'bacon',\n",
       " 'ork',\n",
       " 'sovereignest',\n",
       " 'bruise',\n",
       " 'hamlet',\n",
       " 'leach',\n",
       " 'mote',\n",
       " 'availle',\n",
       " 'returne',\n",
       " 'againe',\n",
       " 'worker',\n",
       " 'dinting',\n",
       " 'paine',\n",
       " 'thro',\n",
       " 'maine',\n",
       " 'faerie',\n",
       " 'til',\n",
       " 'davenant',\n",
       " 'preface',\n",
       " 'gondibert',\n",
       " 'hosmannus',\n",
       " 'nescio',\n",
       " 'vide',\n",
       " 'spencer',\n",
       " 'talus',\n",
       " 'flail',\n",
       " 'threatens',\n",
       " 'jav',\n",
       " 'lins',\n",
       " 'waller',\n",
       " 'commonwealth',\n",
       " 'civitas',\n",
       " 'sentence',\n",
       " 'hobbes',\n",
       " 'mansoul',\n",
       " 'chewing',\n",
       " 'sprat',\n",
       " '---\"',\n",
       " 'draws',\n",
       " 'fulller',\n",
       " 'dryden',\n",
       " 'annus',\n",
       " 'mirabilis',\n",
       " 'aground',\n",
       " 'wantonness',\n",
       " 'fuzzing',\n",
       " 'vents',\n",
       " 'herbert',\n",
       " 'schouten',\n",
       " 'elbe',\n",
       " 'ducat',\n",
       " 'herrings',\n",
       " 'anno',\n",
       " '1652',\n",
       " 'pitferren',\n",
       " 'kinross',\n",
       " 'fierceness',\n",
       " 'strafford',\n",
       " 'bermudas',\n",
       " 'phil',\n",
       " 'trans',\n",
       " '1668',\n",
       " 'primer',\n",
       " 'cowley',\n",
       " '1729',\n",
       " '\"...',\n",
       " 'frequendy',\n",
       " 'insupportable',\n",
       " 'disorder',\n",
       " 'ulloa',\n",
       " 'sylphs',\n",
       " 'petticoat',\n",
       " 'rape',\n",
       " 'dung',\n",
       " 'lime',\n",
       " 'juniper',\n",
       " 'uno',\n",
       " 'troil',\n",
       " '1772',\n",
       " 'nantuckois',\n",
       " 'jefferson',\n",
       " 'memorial',\n",
       " 'minister',\n",
       " 'guarding',\n",
       " 'protecting',\n",
       " 'robbers',\n",
       " 'rodmond',\n",
       " 'suspends',\n",
       " 'attends',\n",
       " 'falconer',\n",
       " 'roofs',\n",
       " 'domes',\n",
       " 'rockets',\n",
       " 'unwieldy',\n",
       " 'dissection',\n",
       " 'aorta',\n",
       " 'gushing',\n",
       " 'paley',\n",
       " 'mammiferous',\n",
       " 'peopling',\n",
       " 'instincts',\n",
       " 'trackless',\n",
       " 'assaulted',\n",
       " 'voracious',\n",
       " 'spiral',\n",
       " 'montgomery',\n",
       " 'paean',\n",
       " 'fatter',\n",
       " 'charles',\n",
       " '1690',\n",
       " 'susan',\n",
       " 'hawthorne',\n",
       " 'bespeak',\n",
       " 'raal',\n",
       " 'berlin',\n",
       " 'gazette',\n",
       " 'eckermann',\n",
       " 'conversations',\n",
       " '1821',\n",
       " 'piping',\n",
       " 'dimmed',\n",
       " 'phospher',\n",
       " 'elizabeth',\n",
       " 'oakes',\n",
       " 'smith',\n",
       " 'amounted',\n",
       " '440',\n",
       " 'agonies',\n",
       " 'endures',\n",
       " 'infuriated',\n",
       " 'rears',\n",
       " 'snaps',\n",
       " 'propelled',\n",
       " 'observers',\n",
       " 'opportunities',\n",
       " 'habitudes',\n",
       " 'offensively',\n",
       " 'artful',\n",
       " 'mischievous',\n",
       " 'debell',\n",
       " '1840',\n",
       " 'october',\n",
       " 'thar',\n",
       " 'bowes',\n",
       " 'os',\n",
       " 'etchings',\n",
       " 'cruize',\n",
       " '1846',\n",
       " 'transactions',\n",
       " 'relate',\n",
       " 'survivors',\n",
       " 'parried',\n",
       " 'journal',\n",
       " 'tyerman',\n",
       " 'boldest',\n",
       " 'persevering',\n",
       " 'breakwater',\n",
       " 'captors',\n",
       " 'biography',\n",
       " 'preble',\n",
       " 'mcculloch',\n",
       " 'reciprocal',\n",
       " 'clews',\n",
       " 'unpublished',\n",
       " 'pedestrians',\n",
       " 'recollect',\n",
       " 'gateways',\n",
       " 'retaking',\n",
       " 'hobomack',\n",
       " 'appliance',\n",
       " 'fuego',\n",
       " 'darwin',\n",
       " \";--'\",\n",
       " '!\\'\"',\n",
       " 'wharton',\n",
       " 'loomings',\n",
       " 'spleen',\n",
       " 'regulating',\n",
       " 'circulation',\n",
       " 'drizzly',\n",
       " 'hypos',\n",
       " 'philosophical',\n",
       " 'cato',\n",
       " 'manhattoes',\n",
       " 'reefs',\n",
       " 'downtown',\n",
       " 'gazers',\n",
       " 'circumambulate',\n",
       " 'corlears',\n",
       " 'coenties',\n",
       " 'whitehall',\n",
       " 'sentinels',\n",
       " 'spiles',\n",
       " 'pier',\n",
       " 'lath',\n",
       " 'counters',\n",
       " 'desks',\n",
       " 'loitering',\n",
       " 'shady',\n",
       " 'inlanders',\n",
       " 'lanes',\n",
       " 'alleys',\n",
       " 'attract',\n",
       " 'dale',\n",
       " 'dreamiest',\n",
       " 'shadiest',\n",
       " 'quietest',\n",
       " 'enchanting',\n",
       " 'saco',\n",
       " 'crucifix',\n",
       " 'mazy',\n",
       " 'tennessee',\n",
       " 'rockaway',\n",
       " 'persians',\n",
       " 'narcissus',\n",
       " 'ungraspable',\n",
       " 'hazy',\n",
       " 'quarrelsome',\n",
       " 'offices',\n",
       " 'abominate',\n",
       " 'toils',\n",
       " 'trials',\n",
       " 'barques',\n",
       " 'schooners',\n",
       " 'broiling',\n",
       " 'buttered',\n",
       " 'judgmatically',\n",
       " 'peppered',\n",
       " 'reverentially',\n",
       " 'idolatrous',\n",
       " 'dotings',\n",
       " 'ibis',\n",
       " 'roasted',\n",
       " 'bake',\n",
       " 'plumb',\n",
       " 'rensselaers',\n",
       " 'randolphs',\n",
       " 'hardicanutes',\n",
       " 'lording',\n",
       " 'tallest',\n",
       " 'decoction',\n",
       " 'seneca',\n",
       " 'stoics',\n",
       " 'testament',\n",
       " 'promptly',\n",
       " 'rub',\n",
       " 'infliction',\n",
       " 'urbane',\n",
       " 'ills',\n",
       " 'monied',\n",
       " 'consign',\n",
       " 'prevalent',\n",
       " 'violate',\n",
       " 'pythagorean',\n",
       " 'commonalty',\n",
       " 'police',\n",
       " 'surveillance',\n",
       " 'programme',\n",
       " 'solo',\n",
       " 'contested',\n",
       " 'election',\n",
       " 'presidency',\n",
       " 'affghanistan',\n",
       " 'managers',\n",
       " 'genteel',\n",
       " 'comedies',\n",
       " 'farces',\n",
       " 'cunningly',\n",
       " 'disguises',\n",
       " 'cajoling',\n",
       " 'unbiased',\n",
       " 'freewill',\n",
       " 'discriminating',\n",
       " 'overwhelming',\n",
       " 'undeliverable',\n",
       " 'itch',\n",
       " 'forbidden',\n",
       " 'ignoring',\n",
       " 'lodges',\n",
       " 'manhatto',\n",
       " 'candidates',\n",
       " 'penalties',\n",
       " 'tyre',\n",
       " 'carthage',\n",
       " 'imported',\n",
       " 'cobblestones',\n",
       " 'bitingly',\n",
       " 'shouldering',\n",
       " 'price',\n",
       " 'fervent',\n",
       " 'asphaltic',\n",
       " 'pavement',\n",
       " 'flinty',\n",
       " 'projections',\n",
       " 'soles',\n",
       " 'cheapest',\n",
       " 'cheeriest',\n",
       " 'invitingly',\n",
       " 'particles',\n",
       " 'peer',\n",
       " 'wailing',\n",
       " 'gnashing',\n",
       " 'entertainment',\n",
       " 'emigrant',\n",
       " 'poverty',\n",
       " 'creak',\n",
       " 'lodgings',\n",
       " 'zephyr',\n",
       " 'hob',\n",
       " 'toasting',\n",
       " 'observest',\n",
       " 'sashless',\n",
       " 'glazier',\n",
       " 'reasonest',\n",
       " 'chinks',\n",
       " 'crannies',\n",
       " 'lint',\n",
       " 'chattering',\n",
       " 'shiverings',\n",
       " 'cob',\n",
       " 'redder',\n",
       " 'orion',\n",
       " 'glitters',\n",
       " 'conservatories',\n",
       " 'president',\n",
       " 'blubbering',\n",
       " 'straggling',\n",
       " 'wainscots',\n",
       " 'reminding',\n",
       " 'oilpainting',\n",
       " 'besmoked',\n",
       " 'defaced',\n",
       " 'unequal',\n",
       " 'crosslights',\n",
       " 'hags',\n",
       " 'delineate',\n",
       " 'bewitched',\n",
       " 'ponderings',\n",
       " 'boggy',\n",
       " 'soggy',\n",
       " 'squitchy',\n",
       " 'froze',\n",
       " 'heath',\n",
       " 'icebound',\n",
       " 'represents',\n",
       " 'horner',\n",
       " 'foundered',\n",
       " 'clubs',\n",
       " 'harvesting',\n",
       " 'hacking',\n",
       " 'horrifying',\n",
       " 'swain',\n",
       " 'blanco',\n",
       " 'sojourning',\n",
       " 'fireplaces',\n",
       " 'duskier',\n",
       " 'cockpits',\n",
       " 'rarities',\n",
       " 'shelves',\n",
       " 'flasks',\n",
       " 'bustles',\n",
       " 'deliriums',\n",
       " 'tumblers',\n",
       " 'cylinders',\n",
       " 'goggling',\n",
       " 'deceitfully',\n",
       " 'tapered',\n",
       " 'pecked',\n",
       " 'footpads',\n",
       " 'shilling',\n",
       " 'examining',\n",
       " 'accommodated',\n",
       " 'unoccupied',\n",
       " 'haint',\n",
       " 'pose',\n",
       " 'whalin',\n",
       " 'decidedly',\n",
       " 'objectionable',\n",
       " 'wander',\n",
       " 'ruminating',\n",
       " 'adorning',\n",
       " 'potatoes',\n",
       " 'sartainty',\n",
       " 'diabolically',\n",
       " 'steaks',\n",
       " 'undress',\n",
       " 'looker',\n",
       " 'rioting',\n",
       " 'feegees',\n",
       " 'tramping',\n",
       " 'bedarned',\n",
       " 'eruption',\n",
       " 'officiating',\n",
       " 'brimmers',\n",
       " 'complained',\n",
       " 'potion',\n",
       " 'colds',\n",
       " 'catarrhs',\n",
       " 'liquor',\n",
       " 'arrantest',\n",
       " 'topers',\n",
       " 'obstreperously',\n",
       " 'aloof',\n",
       " 'desirous',\n",
       " 'hilarity',\n",
       " 'coffer',\n",
       " 'southerner',\n",
       " 'mountaineers',\n",
       " 'alleghanian',\n",
       " 'missed',\n",
       " 'supernaturally',\n",
       " 'congratulate',\n",
       " 'multiply',\n",
       " 'abominated',\n",
       " 'tidiest',\n",
       " 'bedwards',\n",
       " 'tablecloth',\n",
       " 'bump',\n",
       " 'spraining',\n",
       " 'eider',\n",
       " 'yoking',\n",
       " 'rickety',\n",
       " 'whirlwinds',\n",
       " 'knockings',\n",
       " 'dismissed',\n",
       " 'popped',\n",
       " 'cherishing',\n",
       " 'chuckled',\n",
       " 'chuckle',\n",
       " 'mightily',\n",
       " 'bamboozingly',\n",
       " 'overstocked',\n",
       " 'toothpick',\n",
       " 'rayther',\n",
       " 'slanderin',\n",
       " 'farrago',\n",
       " 'sartain',\n",
       " 'mt',\n",
       " 'hecla',\n",
       " 'persist',\n",
       " 'mystifying',\n",
       " 'unsay',\n",
       " 'criminal',\n",
       " 'purty',\n",
       " 'sarmon',\n",
       " 'rips',\n",
       " 'tellin',\n",
       " 'bought',\n",
       " 'balmed',\n",
       " 'curios',\n",
       " 'sellin',\n",
       " 'inions',\n",
       " 'fooling',\n",
       " 'idolators',\n",
       " 'reg',\n",
       " 'lar',\n",
       " 'spliced',\n",
       " 'johnny',\n",
       " 'sprawling',\n",
       " 'arter',\n",
       " 'glim',\n",
       " 'jiffy',\n",
       " 'irresolute',\n",
       " 'vum',\n",
       " 'scrutiny',\n",
       " 'porcupine',\n",
       " 'moccasin',\n",
       " 'ponchos',\n",
       " 'parade',\n",
       " 'rainy',\n",
       " 'commended',\n",
       " 'cobs',\n",
       " 'footfall',\n",
       " 'unlacing',\n",
       " 'blackish',\n",
       " 'plasters',\n",
       " 'inkling',\n",
       " 'crammed',\n",
       " 'scalp',\n",
       " 'mildewed',\n",
       " 'parent',\n",
       " 'nonplussed',\n",
       " 'undressing',\n",
       " 'checkered',\n",
       " 'frogs',\n",
       " 'quaked',\n",
       " 'wrapall',\n",
       " 'dreadnaught',\n",
       " 'fumbled',\n",
       " 'manikin',\n",
       " 'tenpin',\n",
       " 'andirons',\n",
       " 'jambs',\n",
       " 'bricks',\n",
       " 'appropriate',\n",
       " 'applying',\n",
       " 'hastier',\n",
       " 'withdrawals',\n",
       " 'antics',\n",
       " 'devotee',\n",
       " 'extinguishing',\n",
       " 'unceremoniously',\n",
       " 'bagged',\n",
       " 'sportsman',\n",
       " 'woodcock',\n",
       " 'uncomfortableness',\n",
       " 'deliberating',\n",
       " 'puffed',\n",
       " 'sang',\n",
       " 'stammering',\n",
       " 'conjured',\n",
       " 'responses',\n",
       " 'debel',\n",
       " 'flourishing',\n",
       " 'flourishings',\n",
       " 'peddlin',\n",
       " 'sleepe',\n",
       " 'grunted',\n",
       " 'gettee',\n",
       " 'motioning',\n",
       " 'comely',\n",
       " 'insured',\n",
       " 'parti',\n",
       " 'triangles',\n",
       " 'interminable',\n",
       " 'caper',\n",
       " 'supperless',\n",
       " '21st',\n",
       " 'hemisphere',\n",
       " 'sigh',\n",
       " 'ached',\n",
       " 'coaches',\n",
       " 'stockinged',\n",
       " 'slippering',\n",
       " 'misbehaviour',\n",
       " 'unendurable',\n",
       " 'stepmothers',\n",
       " 'misfortunes',\n",
       " 'steeped',\n",
       " 'shudderingly',\n",
       " 'confounding',\n",
       " 'soberly',\n",
       " 'recurred',\n",
       " 'predicament',\n",
       " 'unlock',\n",
       " 'bridegroom',\n",
       " 'clasp',\n",
       " 'hugged',\n",
       " 'rouse',\n",
       " 'snore',\n",
       " 'scratch',\n",
       " 'expostulations',\n",
       " 'unbecomingness',\n",
       " 'matrimonial',\n",
       " 'dawning',\n",
       " 'overture',\n",
       " 'innate',\n",
       " 'compliment',\n",
       " 'civility',\n",
       " 'rudeness',\n",
       " 'toilette',\n",
       " 'dressing',\n",
       " 'donning',\n",
       " 'gaspings',\n",
       " 'booting',\n",
       " 'caterpillar',\n",
       " 'outlandishness',\n",
       " 'manners',\n",
       " 'education',\n",
       " 'undergraduate',\n",
       " 'dreamt',\n",
       " 'cowhide',\n",
       " 'pinched',\n",
       " 'curtains',\n",
       " 'indecorous',\n",
       " 'contented',\n",
       " 'restricting',\n",
       " 'donned',\n",
       " 'lathering',\n",
       " 'unsheathes',\n",
       " 'whets',\n",
       " 'rogers',\n",
       " 'cutlery',\n",
       " 'baton',\n",
       " 'pleasantly',\n",
       " 'bountifully',\n",
       " 'laughable',\n",
       " 'bosky',\n",
       " 'unshorn',\n",
       " 'gowns',\n",
       " 'toasted',\n",
       " 'lingers',\n",
       " 'tarried',\n",
       " 'barred',\n",
       " 'park',\n",
       " 'assurance',\n",
       " 'polish',\n",
       " 'occasioned',\n",
       " 'embarrassed',\n",
       " 'bashfulness',\n",
       " 'duelled',\n",
       " 'winking',\n",
       " 'tastes',\n",
       " 'sheepishly',\n",
       " 'bashful',\n",
       " 'icicle',\n",
       " 'admirer',\n",
       " 'cordially',\n",
       " 'grappling',\n",
       " 'genteelly',\n",
       " 'eschewed',\n",
       " 'undivided',\n",
       " '6',\n",
       " 'circulating',\n",
       " 'nondescripts',\n",
       " 'chestnut',\n",
       " 'jostle',\n",
       " 'regent',\n",
       " 'lascars',\n",
       " 'bombay',\n",
       " 'apollo',\n",
       " 'feegeeans',\n",
       " 'tongatobooarrs',\n",
       " 'erromanggoans',\n",
       " 'pannangians',\n",
       " 'brighggians',\n",
       " 'weekly',\n",
       " 'vermonters',\n",
       " 'stalwart',\n",
       " 'frames',\n",
       " 'felled',\n",
       " 'strutting',\n",
       " 'wester',\n",
       " 'bombazine',\n",
       " 'cloak',\n",
       " 'mow',\n",
       " 'gloves',\n",
       " 'joins',\n",
       " 'outfit',\n",
       " 'waistcoats',\n",
       " 'tract',\n",
       " 'dearest',\n",
       " 'pave',\n",
       " 'eggs',\n",
       " 'patrician',\n",
       " 'parks',\n",
       " 'scraggy',\n",
       " 'scoria',\n",
       " 'herr',\n",
       " 'dowers',\n",
       " 'nieces',\n",
       " 'reservoirs',\n",
       " 'maples',\n",
       " 'bountiful',\n",
       " 'proffer',\n",
       " 'passer',\n",
       " 'cones',\n",
       " 'blossoms',\n",
       " 'superinduced',\n",
       " 'carnation',\n",
       " 'salem',\n",
       " 'sweethearts',\n",
       " 'puritanic',\n",
       " 'quote',\n",
       " 'talbot',\n",
       " 'robert',\n",
       " 'willis',\n",
       " 'ellery',\n",
       " 'walter',\n",
       " 'canny',\n",
       " 'seth',\n",
       " 'gleig',\n",
       " 'eliza',\n",
       " '31st',\n",
       " '1833',\n",
       " 'glazed',\n",
       " 'relatives',\n",
       " 'unhealing',\n",
       " 'sympathetically',\n",
       " 'wounds',\n",
       " 'bleed',\n",
       " 'blanks',\n",
       " 'bordered',\n",
       " 'infidelities',\n",
       " 'gnaw',\n",
       " 'resurrections',\n",
       " 'placelessly',\n",
       " 'goodwin',\n",
       " 'prefix',\n",
       " 'entitle',\n",
       " 'embarks',\n",
       " 'forfeitures',\n",
       " 'immortals',\n",
       " 'unstirring',\n",
       " 'paralysis',\n",
       " 'comforted',\n",
       " 'strive',\n",
       " 'rumor',\n",
       " 'tombs',\n",
       " 'brevet',\n",
       " 'speechlessly',\n",
       " 'chaotic',\n",
       " 'bundling',\n",
       " 'hugely',\n",
       " '8',\n",
       " 'pelted',\n",
       " 'regardful',\n",
       " 'dedicated',\n",
       " 'ministry',\n",
       " 'merging',\n",
       " 'flowering',\n",
       " 'fissures',\n",
       " 'developing',\n",
       " 'peeping',\n",
       " 'february',\n",
       " 'engrafted',\n",
       " 'clerical',\n",
       " 'overshoes',\n",
       " 'ornamental',\n",
       " 'knobs',\n",
       " 'reverential',\n",
       " 'deposited',\n",
       " 'enjoyed',\n",
       " 'courting',\n",
       " 'notoriety',\n",
       " 'withdrawal',\n",
       " 'worldly',\n",
       " 'ties',\n",
       " 'connexions',\n",
       " 'stronghold',\n",
       " 'ehrenbreitstein',\n",
       " 'farings',\n",
       " 'cenotaphs',\n",
       " 'adorned',\n",
       " 'beamed',\n",
       " 'serenest',\n",
       " 'brunt',\n",
       " 'favourable',\n",
       " '9',\n",
       " 'unassuming',\n",
       " 'tolling',\n",
       " 'stanzas',\n",
       " 'pealing',\n",
       " 'exultation',\n",
       " 'deepening',\n",
       " 'complaints',\n",
       " 'confine',\n",
       " 'radiant',\n",
       " 'verse',\n",
       " 'sealine',\n",
       " 'canticle',\n",
       " 'boisterously',\n",
       " 'floods',\n",
       " 'kelpy',\n",
       " 'weed',\n",
       " 'heartedness',\n",
       " 'amittai',\n",
       " 'disobeying',\n",
       " 'hardness',\n",
       " 'flouts',\n",
       " 'skulks',\n",
       " 'easterly',\n",
       " 'skulking',\n",
       " 'prowling',\n",
       " 'hastening',\n",
       " 'policemen',\n",
       " 'valise',\n",
       " 'adieux',\n",
       " 'goods',\n",
       " 'tries',\n",
       " 'joe',\n",
       " 'bigamist',\n",
       " 'adulterer',\n",
       " 'sodom',\n",
       " 'parricide',\n",
       " 'advertised',\n",
       " 'customs',\n",
       " 'mangles',\n",
       " 'rallies',\n",
       " 'scrutinizing',\n",
       " \"?'--'\",\n",
       " 'scent',\n",
       " 'detects',\n",
       " 'penniless',\n",
       " 'passport',\n",
       " 'pauper',\n",
       " 'frontiers',\n",
       " 'prepares',\n",
       " 'charges',\n",
       " 'paves',\n",
       " 'molest',\n",
       " 'counterfeit',\n",
       " 'forger',\n",
       " 'convicts',\n",
       " 'heralding',\n",
       " 'stifling',\n",
       " 'oscillates',\n",
       " 'heeling',\n",
       " 'obliquity',\n",
       " 'contradiction',\n",
       " 'awry',\n",
       " 'groans',\n",
       " 'crookedness',\n",
       " 'hies',\n",
       " 'plungings',\n",
       " 'giddy',\n",
       " 'stupor',\n",
       " 'staunch',\n",
       " 'wrestlings',\n",
       " 'uncheered',\n",
       " 'careening',\n",
       " 'smugglers',\n",
       " 'contraband',\n",
       " 'rebels',\n",
       " 'boatswain',\n",
       " 'lighten',\n",
       " 'jars',\n",
       " 'lethargy',\n",
       " 'stumbling',\n",
       " 'grasps',\n",
       " 'panther',\n",
       " 'gullies',\n",
       " 'lots',\n",
       " 'comest',\n",
       " 'behavior',\n",
       " 'unsolicited',\n",
       " 'confession',\n",
       " 'supplicating',\n",
       " 'howls',\n",
       " 'louder',\n",
       " 'invokingly',\n",
       " 'unreluctantly',\n",
       " 'contenting',\n",
       " 'clamorous',\n",
       " 'pleasing',\n",
       " 'eventual',\n",
       " 'repent',\n",
       " 'describing',\n",
       " 'swarthy',\n",
       " 'lull',\n",
       " 'communing',\n",
       " 'manliest',\n",
       " 'sinner',\n",
       " 'speaker',\n",
       " 'unwelcome',\n",
       " 'truths',\n",
       " 'hostility',\n",
       " 'mission',\n",
       " 'gulfs',\n",
       " 'slantings',\n",
       " 'plummet',\n",
       " 'engulphed',\n",
       " 'breeching',\n",
       " 'bruised',\n",
       " 'murmuring',\n",
       " 'falsehood',\n",
       " 'slights',\n",
       " 'courts',\n",
       " 'enthusiasm',\n",
       " 'support',\n",
       " 'destroys',\n",
       " 'senators',\n",
       " 'acknowledges',\n",
       " 'patriot',\n",
       " 'striven',\n",
       " 'lap',\n",
       " 'regularity',\n",
       " 'whistle',\n",
       " 'fifties',\n",
       " 'hideously',\n",
       " 'uncouthness',\n",
       " 'maim',\n",
       " 'creditor',\n",
       " 'busts',\n",
       " 'graded',\n",
       " 'promontories',\n",
       " 'wooded',\n",
       " 'cannibalistically',\n",
       " 'whilst',\n",
       " 'casement',\n",
       " 'overawing',\n",
       " 'simplicity',\n",
       " 'socratic',\n",
       " 'digester',\n",
       " 'warmed',\n",
       " 'glows',\n",
       " 'hypocrisies',\n",
       " 'repelled',\n",
       " 'hospitalities',\n",
       " 'whereat',\n",
       " 'printing',\n",
       " 'thawed',\n",
       " 'unbiddenly',\n",
       " 'clasped',\n",
       " 'countryman',\n",
       " 'remonstrate',\n",
       " 'silenced',\n",
       " 'deliberated',\n",
       " 'comply',\n",
       " 'prop',\n",
       " 'salamed',\n",
       " 'kissed',\n",
       " 'consciences',\n",
       " 'couples',\n",
       " 'honeymoon',\n",
       " '11',\n",
       " 'nightgown',\n",
       " 'napping',\n",
       " 'affectionately',\n",
       " 'confabulations',\n",
       " 'nappishness',\n",
       " 'wearisome',\n",
       " 'kneepans',\n",
       " 'chilly',\n",
       " 'flatter',\n",
       " 'chilled',\n",
       " 'unmistakably',\n",
       " 'discomforts',\n",
       " 'spark',\n",
       " 'concentrate',\n",
       " 'essences',\n",
       " 'clayey',\n",
       " 'imposed',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist2.hapaxes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, with the language tools you have learned so far, you can easily filter out the words/tokens from a text that have certain characteristics.\n",
    "For example, you can display the tokens that consist of more than 15 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIRCUMNAVIGATION',\n",
       " 'Physiognomically',\n",
       " 'apprehensiveness',\n",
       " 'cannibalistically',\n",
       " 'characteristically',\n",
       " 'circumnavigating',\n",
       " 'circumnavigation',\n",
       " 'circumnavigations',\n",
       " 'comprehensiveness',\n",
       " 'hermaphroditical',\n",
       " 'indiscriminately',\n",
       " 'indispensableness',\n",
       " 'irresistibleness',\n",
       " 'physiognomically',\n",
       " 'preternaturalness',\n",
       " 'responsibilities',\n",
       " 'simultaneousness',\n",
       " 'subterraneousness',\n",
       " 'supernaturalness',\n",
       " 'superstitiousness',\n",
       " 'uncomfortableness',\n",
       " 'uncompromisedness',\n",
       " 'undiscriminating',\n",
       " 'uninterpenetratingly']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words) #sorted in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for words taht have a certain length and occur several times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#14-19teens',\n",
       " '#talkcity_adults',\n",
       " '((((((((((',\n",
       " '........',\n",
       " 'Question',\n",
       " 'actually',\n",
       " 'anything',\n",
       " 'computer',\n",
       " 'cute.-ass',\n",
       " 'everyone',\n",
       " 'football',\n",
       " 'innocent',\n",
       " 'listening',\n",
       " 'remember',\n",
       " 'seriously',\n",
       " 'something',\n",
       " 'together',\n",
       " 'tomorrow',\n",
       " 'watching']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist5 = FreqDist(text5)\n",
    "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams()\n",
    "The NLTK-function bigrams() transforms a sequence of tokens into a sequence of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('am', 'end'), ('end', 'is'), ('is', 'man'), ('man', 'dead')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " list(bigrams([\"am\", \"end\", \"is\", \"man\", \"dead\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocations()\n",
    "\n",
    "Many collocations are high frequency bigrams (or more generally: N-grams). To find such N-grams\n",
    "in a text/corpus, NLTK provides the method collocations():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "text1.collocations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate frequency distribution of word lengths in the text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 19 samples and 260819 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(len(w) for w in text1) #Frequency of word lengths in the text\n",
    "print(fdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(1, 47933), (4, 42345), (2, 38513), (6, 17111), (8, 9966), (9, 6428), (11, 1873), (5, 26597), (7, 14399), (3, 50223), (10, 3528), (12, 1053), (13, 567), (14, 177), (16, 22), (15, 70), (17, 12), (18, 1), (20, 1)])\n"
     ]
    }
   ],
   "source": [
    "print(fdist.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.max() #tokens with length 3 are the most frequent ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50223"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist[3] # there are 50223 tokens with length 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19255882431878046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.freq(3) #they account for nearly 20% of the text of ’Moby Dick’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.gutenberg.fileids())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.corpus.gutenberg refers to the Gutenberg corpus included in the nltk library.\n",
    "\n",
    ".fileids() is a method provided by the nltk library to retrieve a list \n",
    "of file identifiers for the texts available in the specified corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will show how to calculate the length of a text measured in words/tokens using the\n",
    "first text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words(\"austen-emma.txt\")\n",
    "len(emma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words()-function\n",
    " \n",
    ".words() method is used to extract the words from a specific text file within the Gutenberg corpus. The argument passed to the method is the file identifier ('austen-emma.txt' in this case), indicating which text to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg \n",
    "the full prefix nltk.corpus.gutenberg, \n",
    "print(gutenberg.fileids()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of always having to use the full prefix nltk.corpus.gutenberg, it makes sense to import the corpus from the module\n",
    "nltk.corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AN IMPORTANT PROGRAM \n",
    "\n",
    "Now we write a small program that calculates for each text from this corpus, the average word\n",
    "length in characters, sentence length in words and the average occurrence frequency of words\n",
    "in a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(fileid)) # raw: text as string\n",
    "    num_words = len(gutenberg.words(fileid)) # words: list of tokens\n",
    "    num_sents = len(gutenberg.sents(fileid)) # sents: list of sentences\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(round(num_chars/num_words),\n",
    "        round(num_words/num_sents), round(num_words/num_vocab), fileid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for fileid in gutenberg.fileids():: This is a loop that iterates over each file identifier in the Gutenberg corpus.\n",
    "\n",
    "num_chars = len(gutenberg.raw(fileid)): This line calculates the number of characters in the raw text of the specified file using gutenberg.raw(fileid).\n",
    "\n",
    "num_words = len(gutenberg.words(fileid)): This line calculates the number of words in the text of the specified file using gutenberg.words(fileid). \n",
    "\n",
    "num_sents = len(gutenberg.sents(fileid)): This line calculates the number of sentences in the text of the specified file using gutenberg.sents(fileid).\n",
    "\n",
    "num_vocab = len(set(w.lower() for w in gutenberg.words(fileid))): This line calculates the number of unique words (vocabulary) in the text of the specified file. It uses a set comprehension to convert words to lowercase and then calculates the length of the resulting set. The result is stored in the variable num_vocab.\n",
    "\n",
    "print(round(num_chars / num_words), round(num_words / num_sents), round(num_words / num_vocab), fileid): This line prints the rounded average number of characters per word, the rounded average number of words per sentence, the rounded average number of words per unique word in the vocabulary, and the file identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import brown: This line imports the Brown Corpus from the NLTK library. The Brown Corpus is a collection of texts, specifically a sample of American English text from a variety of sources, categorized by genre.\n",
    "\n",
    "brown.categories(): This is a method call on the Brown Corpus object (brown). The .categories() method returns a list of categories or genres present in the Brown Corpus. Each category represents a different genre or topic, such as news, fiction, reviews, etc.\n",
    "\n",
    "print(brown.categories()): This line prints the list of categories to the console. The result will be a list of strings, where each string is a category representing a specific genre or topic in the Brown Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words() and sents() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories=\"news\") #using words()-function to access words belong to news category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brown.words(categories='news'): This is a method call on the Brown Corpus object (brown). The .words() method is used to retrieve a list of words from a specific category in the Brown Corpus. In this case, the category specified is 'news'. This method returns a list of words (tokens) from the specified category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=[\"news\", \"editorial\", \"reviews\"]) #using sents()-function to access words belong to these categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "brown.sents(categories=['news', 'editorial', 'reviews']): This is a method call on the Brown Corpus object (brown). The sents() method is used to retrieve a list of sentences from specific categories in the Brown Corpus. In this case, the categories specified are 'news', 'editorial', and 'reviews'. This method returns a list of sentences, where each sentence is represented as a list of words (tokens).\n",
    "\n",
    "selected_sentences = brown.sents(categories=['news', 'editorial', reviews']): The result of the method call is assigned to the variable selected_sentences. This variable now holds a list of sentences extracted from the specified categories of the Brown Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An importantt program to calculte how often the different modal verbs occur in the newspaper texts of the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389 "
     ]
    }
   ],
   "source": [
    "news_text = brown.words(categories=\"news\")\n",
    "# a list of all token of the news articles\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
    "# a frequency list based of the token list\n",
    "modals =[\"can\",\"could\",\"may\",\"might\",\"must\",\"will\"]\n",
    "# a list of English modals\n",
    "for m in modals:\n",
    "    print(m + \":\", fdist[m], end=\" \" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_text = brown.words(categories=\"news\"): This line extracts all the words (tokens) from the 'news' category of the Brown Corpus and stores them in the variable news_text.\n",
    "\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text): This line creates a frequency distribution (fdist) based on the lowercase version of each word in news_text. It counts the occurrences of each word in the news articles.\n",
    "\n",
    "modals = [\"can\", \"could\", \"may\", \"might\", \"must\", \"will\"]: This line defines a list of English modal verbs.\n",
    "\n",
    "for m in modals:: This line starts a loop that iterates through each modal verb in the list.\n",
    "\n",
    "print(m + \":\", fdist[m], end=\" \"): For each modal verb, this line prints the modal verb, followed by a colon, and then the frequency of that modal verb in the news articles. The end=\" \" argument ensures that the next printed item is on the same line, separated by a space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conditional probability distributions: ConditionalFreqDist()\n",
    "Generally speaking, a conditional frequency distribution (cfd for short) is the frequency with\n",
    "which an event A occurs under the condition that another event B has already occurred.While the FreqDist()-class took a list of words as an argument, the ConditionalFreqDist()-\n",
    "clase takes a list of pairs as an argument:\n",
    "\n",
    "This program is used to investigate the distribution of modals in different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "(genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "genres = [\"news\", \"religion\", \"hobbies\", \"science_fiction\", \"romance\", \"humor\"]\n",
    "modals = [\"can\", \"could\", \"may\", \"might\", \"must\", \"will\"]\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre)): This line creates a Conditional Frequency Distribution (cfd) using a generator expression. It iterates through each genre in the Brown Corpus and each word in the words associated with that genre, creating frequency distributions for each genre and word pair.\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']: This line defines a list of genres for which you want to analyze modal verb frequencies.\n",
    "\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']: This line defines a list of modal verbs for which you want to analyze frequencies.\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals): This line uses the tabulate method of the ConditionalFreqDist object (cfd) to print a table of modal verb frequencies conditioned on different genres. The conditions argument specifies the genres, and the samples argument specifies the modal verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method conditions() :\n",
    "\n",
    "The method conditions() returns a list of all conditions of a cfd and you can\n",
    "use a condition as a key to access the associated frequency distribution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cfd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 1331, 'the': 930, '.': 877, 'of': 515, 'and': 512, 'a': 505, 'to': 463, '``': 343, \"''\": 340, 'in': 334, ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cfd['humor']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical diversity\n",
    "\n",
    "The lexical diversity of a text is defined as the quotient of the number of tokens and the\n",
    "number of types (type/token ratio):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.050197203298673"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(text3) / len(set(text3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate what percentage of the text is taken up by a specific word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * text3.count(\"the\") / len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text3.count(\"the\") counts how many times the word \"the\" appears in the text.\n",
    "\n",
    "len(text3) calculates the total number of words in the text.\n",
    "\n",
    "100 * text3.count(\"the\") / len(text3) computes the percentage by multiplying the count by 100 and dividing by the total number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line imports the stopwords module from the Natural Language Toolkit (NLTK) library. NLTK is a powerful library for working with human language data in Python, and the stopwords module specifically provides a list of common words that are often considered to be of little value in text analysis because they occur frequently across different documents. Examples of stopwords include articles (e.g., \"the,\" \"a,\" \"an\"), prepositions, and conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line retrieves the list of English stopwords from NLTK. The stopwords.words() function takes a language parameter to specify which stopwords list to use, and in this case, 'english' is passed as the parameter, indicating that the function should return the list of English stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a program that calculates the fraction of words in a text that do not belong tp stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735240435097661"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "content_fraction(nltk.corpus.reuters.words())\n",
    "#more than 25% of the texts consist of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def content_fraction(text):: This line defines a function named content_fraction that takes a single argument, text.\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\"): It initializes a list of English stopwords using the NLTK library. These stopwords are commonly occurring words that are often filtered out in text analysis.\n",
    "\n",
    "content = [w for w in text if w.lower() not in stopwords]: It creates a new list called content using a list comprehension. It iterates through each word (w) in the input text (text). It checks if the lowercase version of the word is not in the list of stopwords. If the word is not a stopword, it is included in the content list.\n",
    "\n",
    "return len(content) / len(text): The function returns the ratio of the length of the content list (number of non-stopwords) to the total length of the input text (number of all words). This ratio represents the fraction of content words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A last interesting list-based lexical information source is a lexicon of 8000 first names, where female and male first names are stored in separate lists. The following commands generate a list of all first names that appear in both\n",
    "lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female.txt', 'male.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = nltk.corpus.names\n",
    "###This line imports the names corpus from NLTK. \n",
    "##The nltk.corpus.names module provides access to datasets containing names categorized by gender.\n",
    "names.fileids()\n",
    "###The fileids() method is called on the names object to retrieve a list of file identifiers in the names corpus. \n",
    "##Each file in the corpus represents names associated with a specific gender (e.g., male names or female names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abbey',\n",
       " 'Abbie',\n",
       " 'Abby',\n",
       " 'Addie',\n",
       " 'Adrian',\n",
       " 'Adrien',\n",
       " 'Ajay',\n",
       " 'Alex',\n",
       " 'Alexis',\n",
       " 'Alfie',\n",
       " 'Ali',\n",
       " 'Alix',\n",
       " 'Allie',\n",
       " 'Allyn',\n",
       " 'Andie',\n",
       " 'Andrea',\n",
       " 'Andy',\n",
       " 'Angel',\n",
       " 'Angie',\n",
       " 'Ariel',\n",
       " 'Ashley',\n",
       " 'Aubrey',\n",
       " 'Augustine',\n",
       " 'Austin',\n",
       " 'Averil',\n",
       " 'Barrie',\n",
       " 'Barry',\n",
       " 'Beau',\n",
       " 'Bennie',\n",
       " 'Benny',\n",
       " 'Bernie',\n",
       " 'Bert',\n",
       " 'Bertie',\n",
       " 'Bill',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Blair',\n",
       " 'Blake',\n",
       " 'Bo',\n",
       " 'Bobbie',\n",
       " 'Bobby',\n",
       " 'Brandy',\n",
       " 'Brett',\n",
       " 'Britt',\n",
       " 'Brook',\n",
       " 'Brooke',\n",
       " 'Brooks',\n",
       " 'Bryn',\n",
       " 'Cal',\n",
       " 'Cam',\n",
       " 'Cammy',\n",
       " 'Carey',\n",
       " 'Carlie',\n",
       " 'Carlin',\n",
       " 'Carmine',\n",
       " 'Carroll',\n",
       " 'Cary',\n",
       " 'Caryl',\n",
       " 'Casey',\n",
       " 'Cass',\n",
       " 'Cat',\n",
       " 'Cecil',\n",
       " 'Chad',\n",
       " 'Chris',\n",
       " 'Chrissy',\n",
       " 'Christian',\n",
       " 'Christie',\n",
       " 'Christy',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'Clare',\n",
       " 'Claude',\n",
       " 'Clem',\n",
       " 'Clemmie',\n",
       " 'Cody',\n",
       " 'Connie',\n",
       " 'Constantine',\n",
       " 'Corey',\n",
       " 'Corrie',\n",
       " 'Cory',\n",
       " 'Courtney',\n",
       " 'Cris',\n",
       " 'Daffy',\n",
       " 'Dale',\n",
       " 'Dallas',\n",
       " 'Dana',\n",
       " 'Dani',\n",
       " 'Daniel',\n",
       " 'Dannie',\n",
       " 'Danny',\n",
       " 'Darby',\n",
       " 'Darcy',\n",
       " 'Darryl',\n",
       " 'Daryl',\n",
       " 'Deane',\n",
       " 'Del',\n",
       " 'Dell',\n",
       " 'Demetris',\n",
       " 'Dennie',\n",
       " 'Denny',\n",
       " 'Devin',\n",
       " 'Devon',\n",
       " 'Dion',\n",
       " 'Dionis',\n",
       " 'Dominique',\n",
       " 'Donnie',\n",
       " 'Donny',\n",
       " 'Dorian',\n",
       " 'Dory',\n",
       " 'Drew',\n",
       " 'Eddie',\n",
       " 'Eddy',\n",
       " 'Edie',\n",
       " 'Elisha',\n",
       " 'Emmy',\n",
       " 'Erin',\n",
       " 'Esme',\n",
       " 'Evelyn',\n",
       " 'Felice',\n",
       " 'Fran',\n",
       " 'Francis',\n",
       " 'Frank',\n",
       " 'Frankie',\n",
       " 'Franky',\n",
       " 'Fred',\n",
       " 'Freddie',\n",
       " 'Freddy',\n",
       " 'Gabriel',\n",
       " 'Gabriell',\n",
       " 'Gail',\n",
       " 'Gale',\n",
       " 'Gay',\n",
       " 'Gayle',\n",
       " 'Gene',\n",
       " 'George',\n",
       " 'Georgia',\n",
       " 'Georgie',\n",
       " 'Geri',\n",
       " 'Germaine',\n",
       " 'Gerri',\n",
       " 'Gerry',\n",
       " 'Gill',\n",
       " 'Ginger',\n",
       " 'Glen',\n",
       " 'Glenn',\n",
       " 'Grace',\n",
       " 'Gretchen',\n",
       " 'Gus',\n",
       " 'Haleigh',\n",
       " 'Haley',\n",
       " 'Hannibal',\n",
       " 'Harley',\n",
       " 'Hazel',\n",
       " 'Heath',\n",
       " 'Henrie',\n",
       " 'Hilary',\n",
       " 'Hillary',\n",
       " 'Holly',\n",
       " 'Ike',\n",
       " 'Ikey',\n",
       " 'Ira',\n",
       " 'Isa',\n",
       " 'Isador',\n",
       " 'Isadore',\n",
       " 'Jackie',\n",
       " 'Jaime',\n",
       " 'Jamie',\n",
       " 'Jan',\n",
       " 'Jean',\n",
       " 'Jere',\n",
       " 'Jermaine',\n",
       " 'Jerrie',\n",
       " 'Jerry',\n",
       " 'Jess',\n",
       " 'Jesse',\n",
       " 'Jessie',\n",
       " 'Jo',\n",
       " 'Jodi',\n",
       " 'Jodie',\n",
       " 'Jody',\n",
       " 'Joey',\n",
       " 'Jordan',\n",
       " 'Juanita',\n",
       " 'Jude',\n",
       " 'Judith',\n",
       " 'Judy',\n",
       " 'Julie',\n",
       " 'Justin',\n",
       " 'Karel',\n",
       " 'Kellen',\n",
       " 'Kelley',\n",
       " 'Kelly',\n",
       " 'Kelsey',\n",
       " 'Kerry',\n",
       " 'Kim',\n",
       " 'Kip',\n",
       " 'Kirby',\n",
       " 'Kit',\n",
       " 'Kris',\n",
       " 'Kyle',\n",
       " 'Lane',\n",
       " 'Lanny',\n",
       " 'Lauren',\n",
       " 'Laurie',\n",
       " 'Lee',\n",
       " 'Leigh',\n",
       " 'Leland',\n",
       " 'Lesley',\n",
       " 'Leslie',\n",
       " 'Lin',\n",
       " 'Lind',\n",
       " 'Lindsay',\n",
       " 'Lindsey',\n",
       " 'Lindy',\n",
       " 'Lonnie',\n",
       " 'Loren',\n",
       " 'Lorne',\n",
       " 'Lorrie',\n",
       " 'Lou',\n",
       " 'Luce',\n",
       " 'Lyn',\n",
       " 'Lynn',\n",
       " 'Maddie',\n",
       " 'Maddy',\n",
       " 'Marietta',\n",
       " 'Marion',\n",
       " 'Marlo',\n",
       " 'Martie',\n",
       " 'Marty',\n",
       " 'Mattie',\n",
       " 'Matty',\n",
       " 'Maurise',\n",
       " 'Max',\n",
       " 'Maxie',\n",
       " 'Mead',\n",
       " 'Meade',\n",
       " 'Mel',\n",
       " 'Meredith',\n",
       " 'Merle',\n",
       " 'Merrill',\n",
       " 'Merry',\n",
       " 'Meryl',\n",
       " 'Michal',\n",
       " 'Michel',\n",
       " 'Michele',\n",
       " 'Mickie',\n",
       " 'Micky',\n",
       " 'Millicent',\n",
       " 'Morgan',\n",
       " 'Morlee',\n",
       " 'Muffin',\n",
       " 'Nat',\n",
       " 'Nichole',\n",
       " 'Nickie',\n",
       " 'Nicky',\n",
       " 'Niki',\n",
       " 'Nikki',\n",
       " 'Noel',\n",
       " 'Ollie',\n",
       " 'Page',\n",
       " 'Paige',\n",
       " 'Pat',\n",
       " 'Patrice',\n",
       " 'Patsy',\n",
       " 'Pattie',\n",
       " 'Patty',\n",
       " 'Pen',\n",
       " 'Pennie',\n",
       " 'Penny',\n",
       " 'Perry',\n",
       " 'Phil',\n",
       " 'Pooh',\n",
       " 'Quentin',\n",
       " 'Quinn',\n",
       " 'Randi',\n",
       " 'Randie',\n",
       " 'Randy',\n",
       " 'Ray',\n",
       " 'Regan',\n",
       " 'Reggie',\n",
       " 'Rene',\n",
       " 'Rey',\n",
       " 'Ricki',\n",
       " 'Rickie',\n",
       " 'Ricky',\n",
       " 'Rikki',\n",
       " 'Robbie',\n",
       " 'Robin',\n",
       " 'Ronnie',\n",
       " 'Ronny',\n",
       " 'Rory',\n",
       " 'Ruby',\n",
       " 'Sal',\n",
       " 'Sam',\n",
       " 'Sammy',\n",
       " 'Sandy',\n",
       " 'Sascha',\n",
       " 'Sasha',\n",
       " 'Saundra',\n",
       " 'Sayre',\n",
       " 'Scotty',\n",
       " 'Sean',\n",
       " 'Shaine',\n",
       " 'Shane',\n",
       " 'Shannon',\n",
       " 'Shaun',\n",
       " 'Shawn',\n",
       " 'Shay',\n",
       " 'Shayne',\n",
       " 'Shea',\n",
       " 'Shelby',\n",
       " 'Shell',\n",
       " 'Shelley',\n",
       " 'Sibyl',\n",
       " 'Simone',\n",
       " 'Sonnie',\n",
       " 'Sonny',\n",
       " 'Stacy',\n",
       " 'Sunny',\n",
       " 'Sydney',\n",
       " 'Tabbie',\n",
       " 'Tabby',\n",
       " 'Tallie',\n",
       " 'Tally',\n",
       " 'Tammie',\n",
       " 'Tammy',\n",
       " 'Tate',\n",
       " 'Ted',\n",
       " 'Teddie',\n",
       " 'Teddy',\n",
       " 'Terri',\n",
       " 'Terry',\n",
       " 'Theo',\n",
       " 'Tim',\n",
       " 'Timmie',\n",
       " 'Timmy',\n",
       " 'Tobe',\n",
       " 'Tobie',\n",
       " 'Toby',\n",
       " 'Tommie',\n",
       " 'Tommy',\n",
       " 'Tony',\n",
       " 'Torey',\n",
       " 'Trace',\n",
       " 'Tracey',\n",
       " 'Tracie',\n",
       " 'Tracy',\n",
       " 'Val',\n",
       " 'Vale',\n",
       " 'Valentine',\n",
       " 'Van',\n",
       " 'Vin',\n",
       " 'Vinnie',\n",
       " 'Vinny',\n",
       " 'Virgie',\n",
       " 'Wallie',\n",
       " 'Wallis',\n",
       " 'Wally',\n",
       " 'Whitney',\n",
       " 'Willi',\n",
       " 'Willie',\n",
       " 'Willy',\n",
       " 'Winnie',\n",
       " 'Winny',\n",
       " 'Wynn']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_names = names.words(\"male.txt\")\n",
    "female_names = names.words(\"female.txt\")\n",
    "[w for w in male_names if w in female_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all names ending in ‘a’ are female: only about 1% of male names end with an ‘a’; but\n",
    "a good third of female names do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943\n",
      "29\n",
      "5001\n",
      "1773\n"
     ]
    }
   ],
   "source": [
    "male_a =[w for w in male_names if w[-1] == \"a\"]\n",
    "female_a =[w for w in female_names if w[-1] == \"a\"]\n",
    "print(len(male_names))\n",
    "print(len(male_a))\n",
    "print(len(female_names))\n",
    "print(len(female_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet\n",
    "\n",
    "WordNet is a hierarchically organised dictionary with 155287 entries. For each word, a list\n",
    "of (almost) synonymous other words is stored (synset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('car.n.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(wn.synsets(\"motorcar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import wordnet as wn: This line imports the WordNet module from the NLTK library and assigns it the alias wn. WordNet is a lexical database of the English language that includes information about words and their relationships.\n",
    "\n",
    "wn.synsets(\"motorcar\"): This line calls the synsets function of the WordNet module with the argument \"motorcar.\" The synsets function returns a list of synsets for the specified word. A synset is a set of synonymous words or phrases representing a concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(\"car.n.01\").lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wn.synset(\"car.n.01\"):\n",
    "wn refers to WordNet, which is a lexical database of the English language.\n",
    "synset stands for \"synonym set,\" which is a collection of synonymous words or phrases.\n",
    "The argument \"car.n.01\" is a unique identifier for a particular synset. In this case, it represents the synset for the noun sense of the word \"car.\"\n",
    "\n",
    "The lemma_names() method returns a list of lemma names associated with the synset. A lemma is the base form or dictionary form of a word, and lemma names represent different forms or synonyms of the word in this particular synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"car.n.01\").definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wn.synset('car.n.01'):\n",
    "\n",
    "wn refers to WordNet.\n",
    "synset is used to obtain a synset, and the argument 'car.n.01' is a unique identifier for a specific synset. In this case, it represents the synset for the noun sense of the word \"car\" in WordNet.\n",
    ".definition():\n",
    "\n",
    "After obtaining the synset for \"car.n.01,\" the definition() method is called on it.\n",
    "The definition() method returns a string representing the definition associated with the synset. In other words, it provides a brief explanation or description of the meaning of the word \"car\" in that particular sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset(\"car.n.01\").examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wn.synset('car.n.01'):\n",
    "\n",
    "wn refers to WordNet.\n",
    "synset is used to obtain a synset, and the argument 'car.n.01' is a unique identifier for a specific synset. In this case, it represents the synset for the noun sense of the word \"car\" in WordNet.\n",
    ".examples():\n",
    "\n",
    "After obtaining the synset for \"car.n.01,\" the examples() method is called on it.\n",
    "The examples() method returns a list of example sentences or phrases that illustrate the usage of the word in that particular sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"car\")\n",
    "#Ambiguous words have more than one synset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets(\"car\"):\n",
    "    print(synset.lemma_names())\n",
    "\n",
    "#Synsets represent concepts that are linked in a certain way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyponyms () method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset(\"car.n.01\")\n",
    "motorcar.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wn refers to WordNet.\n",
    "synset is used to obtain a synset, and the argument 'car.n.01' is a unique identifier for a specific synset. In this case, it represents the synset for the noun sense of the word \"car\" in WordNet.\n",
    "The result is assigned to the variable motorcar.\n",
    "\n",
    "\n",
    "motorcar.hyponyms():\n",
    "motorcar is an instance of the synset representing the noun sense of \"car.\"\n",
    "The .hyponyms() method is called on the motorcar synset.\n",
    "The hyponyms() method returns a list of synsets that are more specific or specialized than the current synset. Hyponyms are concepts that are subordinate to the more general concept represented by the original synset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypernyms() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hypernyms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "motorcar:\n",
    "\n",
    "motorcar is an instance of the synset representing the noun sense of \"car,\" obtained earlier.\n",
    ".hypernyms():\n",
    "\n",
    "The .hypernyms() method is called on the motorcar synset.\n",
    "The hypernyms() method returns a list of synsets that are more general or broader in meaning than the concept represented by the original synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar.root_hypernyms() #generates the most general term or root of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "motorcar:\n",
    "\n",
    "motorcar is an instance of the synset representing the noun sense of \"car,\" obtained earlier.\n",
    ".root_hypernyms():\n",
    "\n",
    "The .root_hypernyms() method is called on the motorcar synset.\n",
    "The root_hypernyms() method returns a list of synsets that represent the most general or root-level concepts in the WordNet hierarchy that encompass the concept of the original synset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
